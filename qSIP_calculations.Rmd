---
title: "2H qSIP Calculations"
author: "Tristan Caro"
date: "`r Sys.Date()`"
output:
    html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    toc: yes
    toc_depth: 3
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include = FALSE}
# global knitting options for code rendering
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>")

# global knitting options for automatic saving of all plots as .png and .pdf
knitr::opts_chunk$set(
  dev = c("png", "pdf"),
  dev.args = list(pdf = list(encoding = "WinAnsi", useDingbats = FALSE)),
  fig.keep = "all",
  fig.path = file.path("fig_output", paste0(gsub("\\.[Rr]md", "", knitr::current_input()), "_"))
)
```

# Load packages and functions

```{r, message=FALSE, warning=FALSE}
# Libraries
library(tidyverse)    # CRAN v1.3.1
library(forcats)      # CRAN v0.5.1 
library(isoreader)    # [::NA/NA] v1.3.0 # CRAN v1.3.0
library(isoprocessor) # [github::isoverse/isoprocessor] v0.6.7
library(isotopia)     # [github::isoverse/isotopia] v0.5.8 
library(readxl)       # CRAN v1.3.1 
library(ggsci)        # CRAN v2.9 
library(ggborderline) # [github::wurli/ggborderline] v0.1.0
library(ggrepel)      # CRAN v0.9.1 
library(latex2exp)    # CRAN v0.5.0
library(ggdist)       # CRAN v3.0.0 
library(ggsignif)     # CRAN v0.6.2
library(ggridges)     # CRAN v0.5.3
library(cowplot)      # CRAN v1.1.1

# Sourced Functions
source(file.path("libs", "visualization.R"))
source(file.path("libs", "chromleon.R"))
`%nin%` = Negate(`%in%`)
```

This analysis was run using [isoreader](http://isoreader.kopflab.org) version `r packageVersion("isoreader")` and [isoprocessor](http://isoprocessor.kopflab.org/) version `r packageVersion("isoprocessor")`. 

## Turnover Calculation Function

```{r}
# Define a function to calculate turnover and generation time
# ARGUMENTS:
# FL = 0.005 # tracer strength = 0.5%
# a = 0.5 # incorporation efficiency of 50%
# FT = numeric # resulting isotopic enrichment
# F0 = numeric # beginning isotopic enrichment

gencalc <- function(a, FT, F0, FL, t) {
  case_when(
    t == 0 ~ NA_real_,
    TRUE ~ - (1/t)*(log((FT - a*FL)/(F0 - a*FL)))
  )
}

```


# Load data

## Load IRMS Data

### Load Metadata and Problematic Run Data

```{r}
mtda <- read_csv(file.path("data", "sample_metadata_2021.csv"))
sample_names <- mtda$id1

problematic_runs <- read_xlsx(file.path("data", "IRMS", "problematic_runs.xlsx"))
problematic_analyses <- problematic_runs$Analysis
problematic_analyses_num <- parse_number(problematic_analyses)
```

### Load Peak Map

```{r}
# this information is often maintained in a csv or Excel file
peak_map <- 
  # initial peak map:
  # readxl::read_excel(file.path("data","IRMS", "peak_map_manual_general.xlsx"))
  # corrected peak map
  readxl::read_excel(file.path("data", "IRMS", "peak_maps_resolved.xlsx"))
peak_map
```

### Read raw IRMS data files

```{r, warning=FALSE}
# Set file path(s) to data files, folders or rds collections 
# can be multiple folders or mix of folders and files
# Isoverse will iteratively search subfolders. Huzzah! ^_^
data_path <- file.path("data", "IRMS", "raw_data")
ref_ratio <- get_standard("2H") %>% as.numeric()

# read files
iso_files_raw <- 
  # path to data files
  data_path %>% 
  # read data files in parallel for fast read
  iso_read_continuous_flow() %>%
  # filter out files with read errors (e.g. from aborted analysis)
  iso_filter_files_with_problems()
```

### Process file info & peak table

```{r}
# process file information
iso_files <- iso_files_raw %>% 
  # rename key file info columns
  iso_rename_file_info(analysis = Analysis, id1 = `Identifier 1`, id2 = `Identifier 2`) %>% 
  # parse text info into numbers
  iso_parse_file_info(number = analysis) %>% 
  # process other file information that is specific to the naming conventions
  # of this particular sequence
  iso_mutate_file_info(
    # what is the type of each analysis?
    type = case_when(
      str_detect(id1, "[Zz]ero")      ~ "on_off",
      str_detect(id1, "H3")           ~ "H3_factor",
      str_detect(id1, "F8")           ~ "F8_std",
      str_detect(id1, "F9")           ~ "F9_std",
      TRUE                            ~ "sample"
    ),
    # what was the concentration? (assuming Preparation = concentration or volume)
    concentration = 
      ifelse(type == "std",  
             str_extract(Preparation, "[0-9.]+ ?ng( per |/)uL") %>% 
               parse_number() %>% iso_double_with_units("ng/uL"),
             NA),
    # what folder are the data files in? (assuming folder = sequence)
    folder = basename(dirname(file_path))
  ) %>% 
  # focus only on the relevant file info, discarding the rest
  iso_select_file_info(
    folder, analysis, file_datetime, id1, type, concentration
  ) %>% 
  # add in additional sample metadata (could be any info)
  # note: this would typically be stored in / read from a csv or excel file
  iso_add_file_info(
    read_csv(file.path("data", "sample_metadata_2021.csv")),
    join_by = "id1"
  )

# set peak table from vendor data table with default isodat template
iso_files <- iso_set_peak_table_from_isodat_vendor_data_table(iso_files) %>% 
  # convert units from mV to V for amplitudes and area
  iso_convert_peak_table_units(V = mV, Vs = mVs)

# focus on sample files
sample_files <- iso_filter_files(iso_files, type == "sample")

# # EXCLUDE C26 ALKANE STANDARD
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "C26", negate = TRUE))
# 
# # EXCLUDE SQUALANE STANDARD
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "squalane", negate = TRUE))
# 
# # EXCLUDE BLANKS AND TESTS
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "blank", negate = TRUE))
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "BLANK", negate = TRUE))
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "H2", negate = TRUE))
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "nhex", negate = TRUE))
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "cleaning", negate = TRUE))
# 
# # EXCLUDE BAME STANDARD
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "BAME", negate = TRUE))
# 
# # EXCLUDE F HEAVY
# sample_files <- iso_filter_files(sample_files, str_detect(id1, "F_HEAVY", negate = TRUE))
# 

# Include only sample files
 sample_files <- iso_filter_files(sample_files, id1 %in% sample_names)

# EXCLUDE CRC SAMPLES FROM THIS ANALYSIS
sample_files <- iso_filter_files(sample_files, str_detect(id1, "cri", negate = TRUE))

sample_files <- iso_filter_files(sample_files, str_detect(id1, "cr0", negate = TRUE))

# EXCLUDE RUN WE'VE DEEMED PROBLEMATIC
sample_files <- iso_filter_files(sample_files, analysis %nin% problematic_analyses_num)

```

### Show file information

```{r}
# display file information
iso_files %>% 
  iso_get_file_info() %>% select(-file_id, -folder) %>% 
  iso_make_units_explicit() %>% knitr::kable()
```



### Example chromatograms

```{r "example_chromatograms", fig.width=8, fig.height=8}
# plot an example chromatogram

# sample_files[10] %>% # choosing arbitrary file to plot
# iso_plot_continuous_flow_data(
#   # select data and aesthetics
#   data = c(2), color = id1, panel = id1,
#   # zoom in on time interval
#   time_interval = c(750, 4000),
#   # peak labels for all peaks > 2V
#   peak_bounds = TRUE,
#   peak_marker = FALSE,
#   peak_label = iso_format(rt),
#   #peak_label_size = 3,
#   peak_label_filter = analysis == 5685 & amp2 > 1
# ) + scale_color_npg() + 
#   theme_classic() +
#   theme(strip.background = element_blank())
```


## Import GC-FID Data
```{r}
root <- file.path("data", "gc_fid_data_2020")
file_list = list.files(path = root)
file_list <- file_list %>% 
  str_subset("blank", negate = TRUE) %>% 
  str_subset("Blank", negate = TRUE) %>% 
  str_subset("GCMS-STD", negate = TRUE) %>% 
  str_subset("BLK", negate = TRUE) # remove procedural blanks
sample_names <- file_list %>% 
  str_replace("FID.....", "") %>% 
  str_replace("_100ul-nhex.xls", "") %>% 
  str_replace("_100ul.xls", "")

FID_all_data <-
  tibble(
    sample = sample_names,
    file = file_list,
    data = map(file.path(root, file), read_chromeleon_export),
    injection_details = map(data, "injection_details"),
    integration_results = map(data, "integration_results")
  ) %>% select(-data)
```

### Unnest FID integration details

```{r}
FID_results <- FID_all_data %>% 
  select(-injection_details) %>% 
  unnest(integration_results) %>% 
  filter(nchar(`Peak Name`) > 0) %>% 
  # Specify chain length
  mutate(
    chain = `Peak Name` %>% 
      str_remove("\\d+-OH") %>% 
      parse_number() %>% 
      abs() %>% 
      {paste0("C", .)} %>% 
      factor()
      ) %>% 
  # Rename these columns so we can join the IRMS and FID data
  # Important to specify what data comes from FID!
  rename("sample_id" = sample,
         "compound" = `Peak Name`,
         "rt_FID" = `Retention Time`,
         "area_FID" = `Area`,
         "height_FID" = `Height`,
         "rel_area_FID" = `Relative Area`,
         "rel_height_FID" = `Relative Height`,
         "amount_FID" = `Amount`
         )
```


# Peak Mapping

```{r}
sample_peak_table <- sample_files %>%
  iso_set_peak_table_from_isodat_vendor_data_table() %>%
  iso_get_peak_table() %>% 
  mutate(Analysis = substr(file_id, 1, 7),
         sample_id = substr(file_id, 10,14))
```

## Generate Isoverse Peak Map
```{r}
# Generate an isoverse peak map
peaks_mapped <- sample_peak_table %>%
  iso_map_peaks(peak_map, map_id = Analysis) %>%
  filter(!is.na(compound))
```

## Remove readychecks, blanks, etc. from analysis
```{r}
# Filter based on what is in the sample_names metadata
peaks_mapped <- peaks_mapped %>% 
  filter(sample_id %in% sample_names)

# Filter out runs that are on our problematic_runs list
peaks_mapped <- peaks_mapped %>% 
  filter(Analysis %nin% problematic_runs)
```

## Check problematic peak assignments
```{r}
problem_peaks <- iso_get_problematic_peak_mappings(peaks_mapped)
```


## Export summary of unresolved peaks
```{r}
# Spit out peaks that need to be resolved
peaks_mapped %>%
  ungroup() %>%
  select(Analysis, compound, rt) %>%
  mutate(rt = round(rt, digits=0)) %>%
  pivot_wider(
    names_from = Analysis, 
    names_prefix = "rt:",
    values_from = rt, 
    id_cols = compound, 
    values_fn = function(x) paste(x, collapse = "; ")
  ) %>%
  openxlsx::write.xlsx(file.path("data", "IRMS", "peak_maps_resolve.xlsx"))
```

## Assign incubation params
```{r}

# Add Some Parameters
peaks_mapped <- peaks_mapped %>% 
  group_by(sample_id) %>% 
  mutate(inc_time_d = as.numeric(substr(sample_id, 4,4)),
         inc_time_d_str = paste(inc_time_d, "Days"),
         f_label = 0.005,
         t_series_id = paste0(substr(sample_id, 1,1), 
                              substr(sample_id, 5,5)),
         soil_id = substr(t_series_id, 1,1),
         replicate_id = substr(t_series_id, 2,2),
         terrain = case_when(
           substr(sample_id, 1, 1) == "t" ~ "Niwot Ridge Tundra",
           substr(sample_id, 1 ,1) == "c" ~ "Gordon Gulch Conifer Forest",
           substr(sample_id, 1 ,1) == "m" ~ "Gordon Gulch Meadow",
           substr(sample_id, 1 ,1) == "g" ~ "Marshall Mesa Grassland"))
# 
# inc_time_zero <- peaks_mapped %>% 
#   filter(inc_time_d == 0) %>% 
#   select(sample_id, compound, at2H, t_series_id) %>% 
#   mutate(f_start = at2H) %>% 
#   select(-at2H)
# 
#   

  
```

## Extract zero timepoint values
```{r}
zero_peaks <- peaks_mapped %>%
  filter(inc_time_d_str == "0 Days") %>%
  group_by(t_series_id, compound) %>%
  mutate(n_analyical_reps = n()) %>%
  ungroup() %>%
  # exclude ambiguous peaks
  filter(!is_ambiguous)

zero_peaks_averaged <- zero_peaks %>% 
  group_by(soil_id, compound) %>% 
  summarise(at2H_mn_zero = mean(at2H),
            d2H_mn_zero = mean(d2H),
            area2_mn_zero = mean(area2))
  
zero_peaks_averaged %>% 
  openxlsx::write.xlsx(file.path("data", "IRMS", "zero_peaks_averaged.xlsx"),
                       overwrite = TRUE)

# zero_peaks_averaged <- read_xlsx(file.path("data", "IRMS", "zero_peaks_averaged.xlsx"))


# This is the dataset we will use for generation time calculations
peaks_mapped_with_zeros <- 
  peaks_mapped %>%
  left_join(
    zero_peaks_averaged,
    by = c("soil_id", "compound")
  )
stopifnot(nrow(peaks_mapped_with_zeros) == nrow(peaks_mapped))

# no zero samples
# peaks_mapped_with_zeros %>% filter(is.na(zero_sample_id)) %>% View()

peaks_mapped_with_zeros <- peaks_mapped_with_zeros %>% 
  mutate(
    sample_id = substr(file_id, 10,14)
    )
```


## Join IRMS and GC-FID Data

```{r}
peaks_mapped_with_zeros_FID <- peaks_mapped_with_zeros %>% 
  left_join(FID_results, by = c("sample_id", "compound"))
```

```{r}
# Combine triplicate samples into one
FID_results_trip <- FID_results %>% 
  # Remove standards and fractions
  filter(sample_id != "FAME37.1") %>% 
  filter(sample_id != "TCBAME-2") %>% 
  filter(!str_detect(sample_id, "F1")) %>% 
  filter(!str_detect(sample_id, "F2")) %>% 
  filter(!str_detect(sample_id, "F3")) %>% 
  filter(!str_detect(sample_id, "cr")) %>%
  mutate(sample_rep = str_remove(sample_id, "[xyz]")) %>% 
  group_by(sample_rep, compound) %>%
  # Generate summary stats
  summarize(rel_area_FID_max = max(rel_area_FID, na.rm=TRUE),
            rel_area_FID_min = min(rel_area_FID, na.rm=TRUE),
            rel_height_FID_max = max(rel_height_FID, na.rm=TRUE),
            rel_height_FID_min = min(rel_height_FID, na.rm=TRUE),
            rt_FID = mean(rt_FID, na.rm=TRUE),
            rel_area_FID = mean(rel_area_FID, na.rm=TRUE),
            rel_height_FID = mean(rel_height_FID, na.rm=TRUE)) %>% 
  # Add some basic metadata
  mutate(inc_time = case_when(
    str_detect(sample_rep, "...0") ~ 0,
    str_detect(sample_rep, "...3") ~ 3,
    str_detect(sample_rep, "...7") ~ 7),
    site = case_when(
      str_detect(sample_rep, "c") ~ "Conifer",
      str_detect(sample_rep, "t") ~ "Tundra",
      str_detect(sample_rep, "g") ~ "Grassland",
      str_detect(sample_rep, "m") ~ "Meadow")
  )
```


## Remove standards from gencalcs
```{r}
# Don't want our standard compounds to be used for generation time calculations! x_x
peaks_mapped_with_zeros_analytes <- peaks_mapped_with_zeros_FID %>% 
  filter(str_detect(compound, "STD", negate = TRUE))
```

## Summarize Analytical and Experimental Replicates

```{r}
peaks_mapped_summarized <- peaks_mapped_with_zeros_analytes %>% 
  group_by(terrain, inc_time_d, compound) %>% 
  # FILTER OUT NON BACTERIAL COMPOUNDS
  filter(compound %nin% c("20:0", "22:0", "24:0")) %>% 
  summarize_if(is.numeric, mean, na.rm = TRUE)
```

## Weighted turnover over time course

```{r}
# Take peaks_mapped_summarized (which is the dataframe of IRMS values averaged across
# experimental and analytical replicates).
# Determine the weighted at2H, which is the sum of at2H values times relative area 
# divided by total area.

weighted_time_course <- peaks_mapped_summarized %>% 
  group_by(terrain, inc_time_d) %>%
  # FILTER OUT NON BACTERIAL COMPOUNDS
  filter(compound %nin% c("20:0", "22:0", "24:0")) %>% 
  summarize(at2H_weighted = sum(at2H * rel_area_FID, na.rm = TRUE) / sum(rel_area_FID, na.rm = TRUE))
  
peaks_mapped_summarized <- left_join(peaks_mapped_summarized, 
                                     weighted_time_course,
                                     by = c("terrain", "inc_time_d"))
  
```



## Calculate turnover time
Turnover is calculated as below:

$$
\mu = - \frac{1}{t} \cdot (ln (\frac{F_T - a \cdot F_L}{F_0 - a \cdot F_L})
$$

```
# R code:
gencalc <- function(a, FT, F0, FL, t) {
  case_when(
    t == 0 ~ NA_real_,
    TRUE ~ - (1/t)*(log((FT - a*FL)/(F0 - a*FL)))
  )
}

```

Generation time is related to the turnover rate as below:
$$
g = \frac{ln(2)}{\mu}
$$

```{r}
gen_calc_data <- peaks_mapped_with_zeros_analytes %>% 
  # FILTER OUT NON BACTERIAL COMPOUNDS
  filter(compound %nin% c("20:0", "22:0", "24:0")) %>% 
  mutate(
    f_label = 0.5, # label is 0.5 at%
    a = 0.5, # assimilation efficiency
    t0 = 0, # start time
    f_start = at2H_mn_zero,
    d_t = inc_time_d - t0, # change in time
    f_t = at2H, # restate 2F at time t
    u_d = gencalc(a = 0.5, # Assimilation efficiency
                  FT = f_t, # 2F at time t (at%)
                  F0 = f_start, # 2F at t0 (at%)
                  FL = f_label, # 2F of label (at%)
                  t = d_t), # change in time
    # "negative" turnovers are artifacts of noise and should be zero
    u_d = if_else(u_d < 0, 0, u_d),
    # calculate generation time, converting negatives to zero
    gen_d = if_else(u_d < 0, 0, log(2) / u_d)
  ) %>%
  mutate(
    chain_fctr = compound %>% 
      str_remove("\\d+-OH") %>% 
      parse_number() %>% 
      abs() %>% 
      {paste0("C", .)} %>% 
      factor()
      ) %>% 
  mutate(
    chain_num = compound %>% 
      str_remove("\\d+-OH") %>% 
      parse_number() %>% 
      abs()
  )
```

```{r}

gen_calc_data_summarized <- peaks_mapped_summarized %>% 
  # FILTER OUT NON BACTERIAL COMPOUNDS
  filter(compound %nin% c("20:0", "22:0", "24:0")) %>% 
  mutate(
    f_label = 0.5, # label is 0.5 at%
    a = 0.5, # assimilation efficiency
    t0 = 0, # start time
    f_start = at2H_mn_zero,
    d_t = inc_time_d - t0, # change in time
    f_t = at2H, # restate 2F at time t
    u_d = gencalc(a = 0.5, # Assimilation efficiency
                  FT = f_t, # 2F at time t (at%)
                  F0 = f_start, # 2F at t0 (at%)
                  FL = f_label, # 2F of label (at%)
                  t = d_t), # change in time (d)
    # "negative" turnovers are artifacts of noise and should be zero
    u_d = if_else(u_d < 0, 0, u_d),
    # calculate generation time, converting negatives to zero
    gen_d = if_else(u_d < 0, 0, log(2) / u_d),
  ) %>%
  mutate(
    chain_fctr = compound %>% 
      str_remove("\\d+-OH") %>% 
      parse_number() %>% 
      abs() %>% 
      {paste0("C", .)} %>% 
      factor()
      ) %>% 
  mutate(
    chain_num = compound %>% 
      str_remove("\\d+-OH") %>% 
      parse_number() %>% 
      abs()
  )


weighted_turnover_summarized <- gen_calc_data_summarized %>% 
  group_by(terrain, inc_time_d) %>%
  summarize(u_d_weighted = sum(u_d * rel_area_FID, na.rm = TRUE) / sum(rel_area_FID, na.rm = TRUE),
            gen_d_weighted = sum(gen_d * rel_area_FID, na.rm = TRUE) / sum(rel_area_FID, na.rm = TRUE))

```

```{r join weighted gen calc}
gen_calc_data_summarized_weighted <- left_join(gen_calc_data_summarized, 
                                     weighted_turnover_summarized,
                                     by = c("terrain", "inc_time_d"))

```


# Save Workspace Image
Handoff to paper plotting script

```{r}
# This will take a minute. 
# Uncomment below to save the workspace image.
save.image(file = "cache/qSIP_calculations.RData")
```
